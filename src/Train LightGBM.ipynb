{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3eb8412",
   "metadata": {},
   "source": [
    "# LightGBM Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099c8b9d",
   "metadata": {},
   "source": [
    "##### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b364c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm\n",
    "\n",
    "import re, random, pickle, glob, os, difflib, itertools, logging, warnings, collections\n",
    "warnings.simplefilter(action='ignore')\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from Bio import SeqIO\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "import lightgbm as lgb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf5e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'path\\to\\datafolder' #path to folder containing data files\n",
    "model_path = 'path\\to\\model_folder' #path to saved transformer model\n",
    "model_name = 'name_of_transformer_model' #Find the name of the transformer model from the 'Train Transformer.ipynb'\n",
    "lineages_path = 'name_of_lineages_file' # The lineages file contains information about what subfamily each viral strain belongs to. \n",
    "#Place the lineages file inside the datafolder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02ec284",
   "metadata": {},
   "source": [
    "## 1) Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18efbed3",
   "metadata": {},
   "source": [
    "#### 1.1) Loading in Training Set Genomes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c78f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "viral_genomes = open(os.path.join(datapath, viral_refs)).readlines() ### insert appropriate path to training set FASTA\n",
    "strains = [i[1:-1][:-5] for i in viral_genomes[0::2]]\n",
    "genomes = np.array([re.sub('[^ATCGN]+' ,'', i.replace('\\n', '').upper()) for i in viral_genomes[1::2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a47200",
   "metadata": {},
   "source": [
    "#### 1.2) Create Dictionary (Viral Strain &rarr; Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e0fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "### insert appropriate path to classification TXT\n",
    "genome_cls = open(os.path.join(datapath, 'lineages.txt')).readlines()  \n",
    "\n",
    "strain = 'N\\A'\n",
    "strain_cls = []\n",
    "for i in genome_cls:\n",
    "    if((strain+' ') in i):\n",
    "        cls = i[i.index('>')+2:-1]\n",
    "        try:\n",
    "            cls = cls[:cls.index('pa')]\n",
    "            strain_cls.append((strain, cls))\n",
    "        except:\n",
    "            continue\n",
    "    if('vir_name' in i):\n",
    "        strain = i[10:-1]\n",
    "\n",
    "def Convert(tup, di):\n",
    "    di = dict(tup)\n",
    "    return di\n",
    "      \n",
    "strain_cls = Convert(strain_cls, dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f26f6",
   "metadata": {},
   "source": [
    "#### 1.3) Create Training Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97fc93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame()\n",
    "temp['strains'] = strain_cls.keys()\n",
    "temp['cls'] = strain_cls.values()\n",
    "\n",
    "training_df = pd.DataFrame()\n",
    "training_df['strains'] = strains\n",
    "training_df['genomes'] = genomes\n",
    "training_df = training_df.merge(temp, on='strains')\n",
    "training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca28de",
   "metadata": {},
   "source": [
    "#### 1.4) Gather Genomes by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0978d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_genomes = []; beta_genomes = []; gamma_genomes = []; other_genomes= []\n",
    "\n",
    "for i in df[df.cls=='Alpha'].iterrows():\n",
    "    alpha_genomes.append(i[1].genomes)\n",
    "    alpha_genomes.append(str(Seq(i[1].genomes).reverse_complement()))\n",
    "    \n",
    "for i in df[df.cls=='Beta'].iterrows():\n",
    "    beta_genomes.append(i[1].genomes)\n",
    "    beta_genomes.append(str(Seq(i[1].genomes).reverse_complement()))\n",
    "    \n",
    "for i in df[df.cls=='Gamma'].iterrows():\n",
    "    gamma_genomes.append(i[1].genomes)\n",
    "    gamma_genomes.append(str(Seq(i[1].genomes).reverse_complement()))\n",
    "    \n",
    "for index, row in df.iterrows():\n",
    "    if row['cls'] != 'Alpha' and row['cls'] != 'Beta' and row['cls'] != 'Gamma':\n",
    "        other_genomes.append(row['genomes'])\n",
    "        other_genomes.append(str(Seq(row['genomes']).reverse_complement()))\n",
    "        \n",
    "print(f'# Alpha Genomes: {len(alpha_genomes)} | # Beta Genomes: {len(beta_genomes)} | # Gamma Genomes: {len(gamma_genomes)} | # Other Genomes: {len(other_genomes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95abd6e",
   "metadata": {},
   "source": [
    "#### 1.5) Generate Reads from the Viral Genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044b2fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 150 # length of each read\n",
    "alpha_reads = []\n",
    "num_reads = int(7000/150)\n",
    "for i in alpha_genomes:\n",
    "    read_locs = np.random.randint(0,len(i)-maxlen-1, num_reads)\n",
    "    for j in read_locs:\n",
    "        alpha_reads.append(list(i[j : j+maxlen]))\n",
    "        \n",
    "beta_reads = []\n",
    "num_reads = int(7000/150)\n",
    "for i in beta_genomes:\n",
    "    read_locs = np.random.randint(0,len(i)-maxlen-1, num_reads)\n",
    "    for j in read_locs:\n",
    "        beta_reads.append(list(i[j : j+maxlen]))\n",
    "        \n",
    "gamma_reads = []\n",
    "num_reads = int(7000/150)\n",
    "for i in gamma_genomes:\n",
    "    read_locs = np.random.randint(0,len(i)-maxlen-1, num_reads)\n",
    "    for j in read_locs:\n",
    "        gamma_reads.append(list(i[j : j+maxlen]))\n",
    "        \n",
    "other_reads = []\n",
    "num_reads = int(7000/150)\n",
    "for i in other_genomes:\n",
    "    read_locs = np.random.randint(0,len(i)-maxlen-1, num_reads)\n",
    "    for j in read_locs:\n",
    "        other_reads.append(list(i[j : j+maxlen]))\n",
    "\n",
    "print(f'# Alpha Reads:  {len(alpha_reads)} | # Beta Reads: {len(beta_reads)} | # Gamma Reads:  {len(gamma_reads)} | # Other Reads: {len(other_reads)}')\n",
    "print(f'# Total Reads: {sum([len(alpha_reads),len(beta_reads),len(gamma_reads),len(other_reads)])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2797007",
   "metadata": {},
   "source": [
    "#### 1.7) Tokenize Viral Reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31c105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = \"ACGTN\"\n",
    "mapping = dict(zip(tokens, range(1,len(tokens)+1)))\n",
    "\n",
    "def seqs2cat(seqs, mapping):\n",
    "    def categorical_encode(seq):\n",
    "        seq = [mapping.get(i,5) for i in seq]\n",
    "        return np.array(seq)\n",
    "    vecs = []\n",
    "    for i in seqs:\n",
    "        vecs.append(np.array(categorical_encode(i)))\n",
    "    return np.array(vecs)\n",
    "\n",
    "alpha_tokenized = seqs2cat(alpha_reads, mapping)\n",
    "beta_tokenized = seqs2cat(beta_reads, mapping)\n",
    "gamma_tokenized = seqs2cat(gamma_reads, mapping)\n",
    "other_tokenized = seqs2cat(other_reads,mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea363693",
   "metadata": {},
   "source": [
    "#### 1.8) Load in Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9d5eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "transformer = load_model(os.path.join(model_path, model_name), compile=False)\n",
    "transformer = Model(transformer.input, transformer.layers[-2].output)\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af52b5c",
   "metadata": {},
   "source": [
    "#### 1.9) Encode Tokenized Reads through Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a9508",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_pred = np.mean(transformer.predict(alpha_tokenized, verbose =1), axis=1)\n",
    "beta_pred = np.mean(transformer.predict(beta_tokenized, verbose =1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f50f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_pred = np.mean(transformer.predict(gamma_tokenized, verbose =1), axis=1)\n",
    "other_pred = np.mean(transformer.predict(other_tokenized, verbose =1), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4bced9",
   "metadata": {},
   "source": [
    "## 2) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41d4100",
   "metadata": {},
   "source": [
    "#### 2.1) Organize Viral Encodings and Corresponding Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b640c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "classDict = {0:\"Alpha\",1:\"Beta\",2:\"Gamma\",3:\"Delta\"}\n",
    "pred = np.concatenate((alpha_pred, beta_pred, gamma_pred, other_pred))\n",
    "gt = np.concatenate(([0]*len(alpha_pred), [1]*len(beta_pred), [2]*len(gamma_pred),[3]*len(other_pred)))\n",
    "labelgt = np.concatenate(([\"Alpha\"]*len(alpha_pred), [\"Beta\"]*len(beta_pred), [\"Gamma\"]*len(gamma_pred),[\"Other\"]*len(other_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc3948c",
   "metadata": {},
   "source": [
    "#### 2.2) Creating Training/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ccd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default setting is a 70/30 split\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(pred, gt, test_size=0.3, random_state=42)\n",
    "train_data = lgb.Dataset(X_train,y_train)\n",
    "validate_data = lgb.Dataset(X_validate,y_validate)\n",
    "print(f'Training Set Size: {len(X_train)}')\n",
    "print(f'Validation Set Size: {len(X_validate)}')\n",
    "print(f'Total Size: {len(X_train)+len(X_validate)} (Sanity Check)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb49866",
   "metadata": {},
   "source": [
    "#### 2.3) Fitting LightGBM Model to Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = lgb.LGBMClassifier()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d8618e",
   "metadata": {},
   "source": [
    "## 3) Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa248b63",
   "metadata": {},
   "source": [
    "#### 3.1) Model Performance on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bafb25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_validate)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "val_alpha_accuracy = sum(classifier.predict(X_validate[y_validate==0]) == y_validate[y_validate==0]) / len(X_validate[y_validate==0])\n",
    "val_beta_accuracy = sum(classifier.predict(X_validate[y_validate==1]) == y_validate[y_validate==1]) / len(X_validate[y_validate==1])\n",
    "val_gamma_accuracy = sum(classifier.predict(X_validate[y_validate==2]) == y_validate[y_validate==2]) / len(X_validate[y_validate==2])\n",
    "val_other_accuracy = sum(classifier.predict(X_validate[y_validate==3]) == y_validate[y_validate==3]) / len(X_validate[y_validate==3])\n",
    "\n",
    "accuracy_scores=np.array([[val_alpha_accuracy,val_beta_accuracy,val_gamma_accuracy,val_other_accuracy]])\n",
    "accuracies = pd.DataFrame(accuracy_scores, columns=[\"Alpha\",\"Beta\",\"Gamma\",\"Other\"])\n",
    "accuracies.index = [\"Accuracy\"]\n",
    "display(accuracies)\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_palette(\"rocket\")\n",
    "sns.barplot(data=accuracies)\n",
    "sns.set(rc={'figure.figsize':(7,5)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ecfb9b",
   "metadata": {},
   "source": [
    "## 4) Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0ed5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(classifier, os.path.join(model_path, 'LightGBM_Model.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
